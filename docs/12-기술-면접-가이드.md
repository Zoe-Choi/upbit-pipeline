# 데이터 엔지니어링 기술 면접 가이드

> 이 프로젝트에서 사용한 모든 기술 개념을 면접 준비용으로 정리한 문서입니다.
> 대학생도 이해할 수 있도록 쉬운 언어로 설명하고, 실무 관점의 심화 내용까지 다룹니다.

## 목차

1. [컴퓨터 과학 기초](#1-컴퓨터-과학-기초)
2. [네트워크 기초](#2-네트워크-기초)
3. [분산 시스템 기초](#3-분산-시스템-기초)
4. [메시지 큐와 스트리밍](#4-메시지-큐와-스트리밍-kafka)
5. [스트림 처리](#5-스트림-처리-flink)
6. [데이터베이스](#6-데이터베이스-clickhouse)
7. [객체 스토리지](#7-객체-스토리지-minio--paimon)
8. [컨테이너와 오케스트레이션](#8-컨테이너와-오케스트레이션)
9. [Python 심화](#9-python-심화)
10. [실무 운영 튜닝](#10-실무-운영-튜닝)
11. [예상 면접 질문](#11-예상-면접-질문)

---

# 1. 컴퓨터 과학 기초

## 1.1 동기(Synchronous) vs 비동기(Asynchronous)

### 쉬운 설명

**동기**: 커피숍에서 주문하고 커피 나올 때까지 **기다리는 것**
**비동기**: 주문하고 진동벨 받아서 **다른 일 하다가** 벨 울리면 가져오는 것

### 기술적 설명

```python
# 동기 방식: 순차적 실행
def sync_example():
    result1 = fetch_data_from_db()      # 3초 대기
    result2 = fetch_data_from_api()     # 2초 대기
    return result1, result2             # 총 5초 소요

# 비동기 방식: 동시 실행
async def async_example():
    task1 = asyncio.create_task(fetch_data_from_db())   # 시작
    task2 = asyncio.create_task(fetch_data_from_api())  # 시작
    result1, result2 = await asyncio.gather(task1, task2)  # 총 3초 소요
```

### 이 프로젝트에서의 적용

```python
# WebSocket 메시지 수신 - 비동기
async def _listen(self) -> None:
    async for message in self._websocket:  # 비동기 이터레이터
        await self._handle_message(message)
```

**왜 비동기를 사용했나?**
- WebSocket은 언제 메시지가 올지 모름 (I/O 바운드)
- 기다리는 동안 다른 작업 가능
- 단일 스레드로 수천 개 연결 처리 가능

### 면접 포인트

> **Q: 동기와 비동기의 차이점은?**
> 
> A: 동기는 작업이 완료될 때까지 호출자가 블로킹되고, 비동기는 작업을 시작만 하고 
> 완료 알림(콜백, await)을 받습니다. I/O 바운드 작업에서 비동기가 효율적입니다.

> **꼬리질문: 그럼 언제 동기를 쓰나요?**
> 
> A: CPU 바운드 작업(계산 집약적), 순서가 중요한 트랜잭션, 
> 디버깅이 중요한 경우 동기가 더 적합합니다.

---

## 1.2 블로킹(Blocking) vs 논블로킹(Non-blocking)

### 쉬운 설명

**블로킹**: 전화했는데 상대방이 받을 때까지 **아무것도 못 함**
**논블로킹**: 문자 보내고 **바로 다른 일 함**, 답장 오면 확인

### 기술적 설명

```python
# 블로킹 I/O
data = socket.recv(1024)  # 데이터 올 때까지 멈춤

# 논블로킹 I/O
socket.setblocking(False)
try:
    data = socket.recv(1024)  # 데이터 없으면 즉시 예외
except BlockingIOError:
    pass  # 다른 작업 수행
```

### 이 프로젝트에서의 적용

```python
# Kafka Producer - 논블로킹
self._producer.produce(
    topic=topic,
    value=serialized_value,
    ...
)
self._producer.poll(0)  # 논블로킹 폴링 (즉시 반환)
```

**왜 논블로킹?**
- `produce()`는 내부 버퍼에 추가만 하고 즉시 반환
- 실제 전송은 librdkafka 백그라운드 스레드가 담당
- 메인 스레드는 계속 메시지 처리 가능

### 동기/비동기 vs 블로킹/논블로킹

| | 블로킹 | 논블로킹 |
|---|--------|----------|
| **동기** | 전통적 I/O | 폴링 방식 |
| **비동기** | 거의 없음 | 이벤트 드리븐 |

---

## 1.3 프로세스 vs 스레드 vs 코루틴

### 쉬운 설명

- **프로세스**: 완전히 **독립된 프로그램** (크롬 탭 각각)
- **스레드**: 프로그램 안의 **작업자** (한 탭 안에서 여러 일)
- **코루틴**: **협력적 멀티태스킹** (한 작업자가 일 나눠서)

### 메모리 관점

```
프로세스 A          프로세스 B
┌──────────┐       ┌──────────┐
│ 코드     │       │ 코드     │
│ 데이터   │       │ 데이터   │  ← 완전히 분리
│ 힙       │       │ 힙       │
│ 스택     │       │ 스택     │
└──────────┘       └──────────┘

프로세스
┌─────────────────────────────────┐
│ 코드, 데이터, 힙 (공유)           │
│ ┌────────┐ ┌────────┐ ┌────────┐│
│ │스택 A  │ │스택 B  │ │스택 C  ││  ← 스레드별 스택만 분리
│ └────────┘ └────────┘ └────────┘│
└─────────────────────────────────┘
```

### 이 프로젝트에서의 선택

```python
# 코루틴 사용 (asyncio)
async def handle_ticker(self, message: TickerMessage) -> None:
    kafka_message = message.to_kafka_message()
    self._producer.produce_ticker(kafka_message)
```

**왜 코루틴?**
1. **경량**: 스레드보다 1000배 가벼움 (KB vs MB)
2. **GIL 우회**: Python GIL 영향 없음
3. **컨텍스트 스위칭 비용 낮음**: 커널 개입 없음

### 면접 포인트

> **Q: Python에서 멀티스레딩이 왜 문제인가요?**
> 
> A: GIL(Global Interpreter Lock) 때문에 한 번에 하나의 스레드만 
> Python 바이트코드를 실행할 수 있습니다. CPU 바운드 작업에서는 
> 멀티프로세싱이, I/O 바운드에서는 asyncio가 더 효율적입니다.

> **꼬리질문: GIL이 있는데도 왜 스레드를 쓰나요?**
> 
> A: I/O 작업 중에는 GIL이 해제됩니다. 네트워크 I/O나 파일 I/O에서는 
> 스레드가 여전히 유용합니다. 또한 C 확장 라이브러리(numpy 등)는 
> GIL을 해제하고 작업합니다.

---

## 1.4 시간 복잡도와 공간 복잡도

### 쉬운 설명

- **시간 복잡도**: 입력이 커지면 **시간이 얼마나 더 걸리나?**
- **공간 복잡도**: 입력이 커지면 **메모리를 얼마나 더 쓰나?**

### Big-O 표기법

| 표기법 | 이름 | 예시 | 10개 → 1000개 |
|--------|------|------|---------------|
| O(1) | 상수 | 배열 인덱스 접근 | 1 → 1 |
| O(log n) | 로그 | 이진 탐색 | 3 → 10 |
| O(n) | 선형 | 배열 순회 | 10 → 1000 |
| O(n log n) | 선형 로그 | 정렬 | 33 → 10000 |
| O(n²) | 이차 | 버블 정렬 | 100 → 1000000 |

### 이 프로젝트에서의 복잡도 분석

```python
# O(1) - 딕셔너리 조회
market = data.get("market")

# O(n) - 필수 필드 검증 (n = 필드 수)
for field in cls.REQUIRED_FIELDS:  # n번
    if field not in data:          # O(1) dict lookup
        raise MissingFieldError(field, data)

# O(k) - Orderbook 단위 파싱 (k = 호가 레벨 수)
units = [
    OrderbookUnit.from_dict(unit)
    for unit in data.get("orderbook_units", [])
]
```

### 실무에서의 최적화

```python
# 비효율: O(n) 매번 검색
def find_market(markets, code):
    for m in markets:
        if m.code == code:
            return m

# 효율: O(1) 딕셔너리 사용
market_dict = {m.code: m for m in markets}  # 전처리
def find_market(code):
    return market_dict.get(code)
```

---

## 1.5 직렬화(Serialization)와 역직렬화(Deserialization)

### 쉬운 설명

- **직렬화**: 객체를 **문자열/바이트로 변환** (택배 포장)
- **역직렬화**: 문자열/바이트를 **객체로 복원** (택배 개봉)

### 왜 필요한가?

```
Python 객체 (메모리)          네트워크/파일
┌────────────────┐           ┌────────────────┐
│ TickerMessage  │  ──JSON─→ │ {"market":...} │
│   .market      │           │                │
│   .price       │  ←─JSON── │                │
└────────────────┘           └────────────────┘
```

- 네트워크로 전송하려면 바이트 스트림 필요
- 프로세스 간 통신, 파일 저장에도 필요

### 이 프로젝트에서의 구현

```python
class JSONSerializer:
    def __call__(self, obj: Dict[str, Any], ctx=None) -> bytes:
        return json.dumps(obj, ensure_ascii=False).encode("utf-8")

class JSONDeserializer:
    def __call__(self, data: bytes, ctx=None) -> Dict[str, Any]:
        return json.loads(data.decode("utf-8"))
```

### 직렬화 포맷 비교

| 포맷 | 장점 | 단점 | 사용처 |
|------|------|------|--------|
| JSON | 가독성, 호환성 | 크기 큼, 느림 | REST API |
| Protocol Buffers | 빠름, 작음 | 스키마 필요 | gRPC |
| Avro | 스키마 진화 | 복잡 | Kafka |
| MessagePack | JSON 호환, 작음 | 바이너리 | 캐시 |

### 면접 포인트

> **Q: JSON 대신 다른 포맷을 쓰면 어떤 이점이 있나요?**
> 
> A: Protocol Buffers는 바이너리라 JSON보다 2-5배 작고 10배 빠릅니다.
> 단, 스키마 관리가 필요하고 디버깅이 어렵습니다. 내부 서비스 간 통신에 적합합니다.

---

# 2. 네트워크 기초

## 2.1 HTTP vs WebSocket

### 쉬운 설명

- **HTTP**: 편지 주고받기 (매번 우체국 가야 함)
- **WebSocket**: 전화 통화 (한 번 연결하면 계속 대화)

### 기술적 차이

```
HTTP (Request-Response)
클라이언트 ──요청──→ 서버
클라이언트 ←──응답── 서버
(연결 종료)
클라이언트 ──요청──→ 서버  ← 다시 연결
클라이언트 ←──응답── 서버

WebSocket (Full-duplex)
클라이언트 ──HTTP Upgrade──→ 서버
클라이언트 ←─────────────── 서버 (101 Switching Protocols)
         ↕ 양방향 통신 ↕
클라이언트 ←──데이터──→ 서버  ← 연결 유지
```

### 이 프로젝트에서의 사용

```python
# WebSocket 연결
self._websocket = await websockets.connect(
    self._config.websocket_url,
    ping_interval=30,    # 30초마다 핑
    ping_timeout=10,     # 10초 내 응답 없으면 재연결
)

# 메시지 수신 (서버가 푸시)
async for message in self._websocket:
    await self._handle_message(message)
```

**왜 WebSocket?**
- 업비트 실시간 시세: 초당 수십~수백 메시지
- HTTP로 하면 매번 연결 비용 발생
- WebSocket은 한 번 연결로 지속적 수신

### 면접 포인트

> **Q: WebSocket의 단점은?**
> 
> A: 상태 유지가 필요해 서버 확장이 어렵습니다. 로드밸런서에서
> sticky session이 필요하고, 연결이 끊어지면 재연결 로직이 필요합니다.

> **꼬리질문: 그럼 어떻게 확장하나요?**
> 
> A: Redis Pub/Sub이나 Kafka를 백엔드로 사용해 상태를 공유하거나,
> 클라이언트별로 서버를 할당하는 consistent hashing을 사용합니다.

---

## 2.2 TCP vs UDP

### 쉬운 설명

- **TCP**: 등기우편 (도착 확인, 순서 보장, 느림)
- **UDP**: 일반우편 (확인 없음, 순서 모름, 빠름)

### 특성 비교

| 특성 | TCP | UDP |
|------|-----|-----|
| 연결 | 연결 지향 (3-way handshake) | 비연결 |
| 신뢰성 | 보장 (재전송) | 미보장 |
| 순서 | 보장 | 미보장 |
| 속도 | 상대적 느림 | 빠름 |
| 사용처 | HTTP, 데이터베이스 | 스트리밍, 게임 |

### 3-Way Handshake (TCP 연결)

```
클라이언트                    서버
    │                          │
    │──── SYN (seq=x) ────────→│  "연결하자"
    │                          │
    │←── SYN-ACK (seq=y, ack=x+1) ──│  "알겠어, 나도"
    │                          │
    │──── ACK (ack=y+1) ──────→│  "확인"
    │                          │
    │      연결 완료           │
```

### 이 프로젝트에서

- **Kafka**: TCP 사용 (메시지 손실 불가)
- **WebSocket**: TCP 위에 구축
- **ClickHouse**: TCP 사용 (데이터 정확성 필수)

---

## 2.3 HTTP/1.1 vs HTTP/2 vs HTTP/3

### 쉬운 설명

- **HTTP/1.1**: 한 줄 서기 (요청 하나씩)
- **HTTP/2**: 여러 창구 (한 연결에 여러 요청)
- **HTTP/3**: 더 빠른 창구 (UDP 기반)

### 기술적 차이

```
HTTP/1.1
┌───────────────────────────┐
│ 연결 1: 요청A → 응답A      │
│ 연결 2: 요청B → 응답B      │  ← 여러 연결 필요
│ 연결 3: 요청C → 응답C      │
└───────────────────────────┘

HTTP/2 (Multiplexing)
┌───────────────────────────┐
│ 연결 1:                    │
│   스트림 1: 요청A → 응답A  │
│   스트림 2: 요청B → 응답B  │  ← 하나의 연결
│   스트림 3: 요청C → 응답C  │
└───────────────────────────┘
```

---

# 3. 분산 시스템 기초

## 3.1 CAP 정리

### 쉬운 설명

분산 시스템에서 **세 가지 중 두 가지만** 보장 가능:

- **C**onsistency (일관성): 모든 노드가 같은 데이터
- **A**vailability (가용성): 항상 응답 가능
- **P**artition tolerance (분할 허용): 네트워크 분리에도 동작

### 실제 선택

| 시스템 | 선택 | 설명 |
|--------|------|------|
| 전통 RDBMS | CA | 네트워크 분할 시 중단 |
| Kafka | CP | 일관성 우선, 일시적 불가용 허용 |
| Cassandra | AP | 가용성 우선, 일시적 불일치 허용 |

### 이 프로젝트에서

```python
# Kafka - CP 선택
"acks": "all"  # 모든 복제본 확인 (일관성)
# 리더 다운 시 잠시 불가용 가능
```

### 면접 포인트

> **Q: CAP에서 왜 세 가지를 다 가질 수 없나요?**
> 
> A: 네트워크 분할이 발생하면 선택해야 합니다. 분할된 노드의 
> 요청을 거부하면 가용성 포기(CP), 허용하면 일관성 포기(AP).
> 현실적으로 네트워크 분할은 피할 수 없어서 P는 필수입니다.

---

## 3.2 일관성 모델

### 강한 일관성 (Strong Consistency)

```
쓰기 완료 후 모든 읽기가 최신 데이터 반환

시간 ──→
Writer: WRITE(x=1) ────────────────→
Reader1:            READ(x) → 1
Reader2:                      READ(x) → 1
```

### 최종 일관성 (Eventual Consistency)

```
쓰기 후 일정 시간 내에 일관성 보장

시간 ──→
Writer: WRITE(x=1) ────────────────→
Reader1:            READ(x) → 0  (아직 전파 안 됨)
Reader2:                      READ(x) → 1  (전파됨)
                    ↓
                 언젠가 모두 1
```

### 이 프로젝트에서

- **ClickHouse**: 최종 일관성 (비동기 복제)
- **Kafka `acks=all`**: 강한 일관성

---

## 3.3 복제(Replication)

### 쉬운 설명

데이터를 **여러 곳에 복사**해서 안전하게 보관

### 복제 방식

```
동기 복제 (Synchronous)
┌──────┐    쓰기    ┌──────┐
│Leader│───────────→│Follower│
│      │←──확인────│      │
│      │    쓰기    │      │
│      │───────────→│Follower2│
│      │←──확인────│      │
└──────┘           └──────┘
  ↓
응답 (느림, 안전)

비동기 복제 (Asynchronous)
┌──────┐    쓰기    ┌──────┐
│Leader│───────────→│Follower│ (언젠가)
│      │            │      │
└──────┘           └──────┘
  ↓
응답 (빠름, 위험)
```

### Kafka의 복제

```
토픽: upbit-ticker, 복제본 3개

Broker 1        Broker 2        Broker 3
┌──────────┐   ┌──────────┐   ┌──────────┐
│Partition0│   │Partition0│   │Partition0│
│ (Leader) │   │(Follower)│   │(Follower)│
└──────────┘   └──────────┘   └──────────┘
     │              ↑              ↑
     └──────────────┴──────────────┘
              복제
```

```python
# 복제 설정
replication_factor = 3  # 3개 복제본
min.insync.replicas = 2  # 최소 2개 동기화 필요
```

---

## 3.4 파티셔닝(Sharding)

### 쉬운 설명

데이터를 **여러 조각으로 나눠서** 분산 저장

### 파티셔닝 전략

```
1. 키 기반 파티셔닝
hash(market) % partition_count

"KRW-BTC" → hash → 12345 % 3 = 0 → Partition 0
"KRW-ETH" → hash → 67890 % 3 = 1 → Partition 1

2. 범위 기반 파티셔닝
A-M → Partition 0
N-Z → Partition 1
```

### 이 프로젝트에서

```python
# Kafka 메시지 키로 파티션 결정
self.produce(
    topic=self._config.topic_ticker,
    value=message,
    key=market,  # "KRW-BTC" → 특정 파티션
)
```

**왜 market을 키로?**
- 같은 코인은 항상 같은 파티션 → 순서 보장
- 파티션별 병렬 처리 가능

### 면접 포인트

> **Q: 파티션 키를 잘못 선택하면 어떤 문제가?**
> 
> A: 핫 파티션(hot partition) 문제가 발생합니다. 예를 들어 
> user_id를 키로 하는데 대형 사용자가 있으면 특정 파티션에 
> 부하가 집중됩니다. 복합 키나 salt를 추가해 분산합니다.

---

# 4. 메시지 큐와 스트리밍 (Kafka)

## 4.1 Kafka 기본 개념

### 쉬운 설명

- **토픽(Topic)**: 메시지 카테고리 (우편함)
- **파티션(Partition)**: 토픽의 분할 (우편함 칸막이)
- **프로듀서(Producer)**: 메시지 보내는 쪽 (편지 쓰는 사람)
- **컨슈머(Consumer)**: 메시지 받는 쪽 (편지 읽는 사람)
- **브로커(Broker)**: Kafka 서버 (우체국)

### 아키텍처

```
Producer                      Kafka Cluster                    Consumer
                         ┌───────────────────┐
   ┌──────┐             │   Topic: ticker   │             ┌──────┐
   │ App  │──메시지──→  │ ┌─────┐ ┌─────┐   │  ──메시지──→│ App  │
   └──────┘             │ │P0   │ │P1   │   │             └──────┘
                         │ └─────┘ └─────┘   │
                         │   Broker 1~3      │
                         └───────────────────┘
```

### Kafka vs 전통 메시지 큐

| 특성 | Kafka | RabbitMQ |
|------|-------|----------|
| 모델 | 로그 기반 | 큐 기반 |
| 메시지 유지 | 설정된 기간 보관 | 소비 후 삭제 |
| 재처리 | 오프셋으로 가능 | 불가능 |
| 처리량 | 매우 높음 | 높음 |
| 순서 | 파티션 내 보장 | 큐 내 보장 |

### 면접 포인트

> **Q: Kafka를 왜 선택했나요?**
> 
> A: 세 가지 이유입니다.
> 1. 재처리 가능 - 오류 시 과거 데이터 다시 처리
> 2. 높은 처리량 - 초당 수십만 메시지
> 3. 내구성 - 디스크 기반 저장, 복제

---

## 4.2 KRaft 모드

### 쉬운 설명

예전에는 Kafka가 **Zookeeper**라는 별도 시스템에 의존했는데,
이제는 **자체적으로** 클러스터 관리 (KRaft)

### 차이점

```
기존 (Zookeeper)
┌────────────────────────────────────┐
│          Zookeeper 클러스터         │  ← 별도 관리 필요
└────────────────────────────────────┘
              ↓
┌──────┐  ┌──────┐  ┌──────┐
│Broker│  │Broker│  │Broker│
└──────┘  └──────┘  └──────┘

KRaft 모드
┌──────────────────────────────────────────┐
│ ┌──────┐  ┌──────┐  ┌──────┐            │
│ │Broker│  │Broker│  │Broker│            │
│ │+Ctrl │  │+Ctrl │  │+Ctrl │ ← 컨트롤러 내장
│ └──────┘  └──────┘  └──────┘            │
└──────────────────────────────────────────┘
```

### 이 프로젝트 설정

```yaml
# docker-compose.yml
kafka-1:
  environment:
    KAFKA_PROCESS_ROLES: broker,controller  # 두 역할 동시에
    KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-1:9093,2@kafka-2:9093,3@kafka-3:9093
```

---

## 4.3 Producer 설정 (실무 튜닝)

### 주요 설정

```python
self._producer = Producer({
    "bootstrap.servers": "kafka-1:9092,kafka-2:9092,kafka-3:9092",
    
    # 안정성 설정
    "acks": "all",           # 모든 복제본 확인 (1: 리더만, 0: 확인 안 함)
    "retries": 3,            # 실패 시 재시도
    
    # 성능 설정
    "linger.ms": 10,         # 배치를 위해 대기 (0: 즉시 전송)
    "batch.size": 16384,     # 배치 크기 (16KB)
    "compression.type": "snappy",  # 압축 (none, gzip, lz4, zstd)
    
    # 버퍼 설정
    "buffer.memory": 33554432,     # 버퍼 크기 (32MB)
    "max.block.ms": 60000,         # 버퍼 풀 대기 시간
})
```

### 설정 트레이드오프

| 설정 | 높으면 | 낮으면 |
|------|--------|--------|
| `acks` | 안전, 느림 | 위험, 빠름 |
| `linger.ms` | 처리량↑, 지연↑ | 처리량↓, 지연↓ |
| `batch.size` | 효율적, 메모리↑ | 비효율, 메모리↓ |

### 면접 포인트

> **Q: acks=all과 acks=1의 차이는?**
> 
> A: `acks=1`은 리더만 확인하고 응답, 리더 크래시 시 데이터 손실 가능.
> `acks=all`은 모든 ISR(In-Sync Replica) 확인 후 응답, 안전하지만 
> 레이턴시 증가. min.insync.replicas와 함께 설정해야 의미 있음.

> **꼬리질문: 그럼 acks=0은 언제 쓰나요?**
> 
> A: 로그나 메트릭처럼 일부 손실이 허용되고 처리량이 중요한 경우.
> 또는 재시도 로직이 상위에 있는 경우에 사용합니다.

---

## 4.4 Consumer와 Consumer Group

### 쉬운 설명

- **Consumer Group**: 같은 데이터를 **나눠서** 처리하는 컨슈머 그룹
- **파티션 할당**: 각 파티션은 그룹 내 **하나의 컨슈머만** 처리

### 동작 방식

```
토픽: upbit-ticker (3 파티션)

Consumer Group A (3 컨슈머)
┌────┐  ┌────┐  ┌────┐
│ P0 │  │ P1 │  │ P2 │
└──┬─┘  └──┬─┘  └──┬─┘
   ↓       ↓       ↓
┌────┐  ┌────┐  ┌────┐
│C1  │  │C2  │  │C3  │  ← 각자 1개 파티션
└────┘  └────┘  └────┘

Consumer Group B (1 컨슈머)
┌────┐  ┌────┐  ┌────┐
│ P0 │  │ P1 │  │ P2 │
└──┬─┘  └──┬─┘  └──┬─┘
   └───────┼───────┘
           ↓
        ┌────┐
        │C1  │  ← 혼자 3개 파티션
        └────┘
```

### 오프셋 관리

```
파티션 0: [msg0, msg1, msg2, msg3, msg4, msg5, ...]
                                ↑
                         현재 오프셋 = 4
                         (msg0~msg3 처리 완료)
```

---

# 5. 스트림 처리 (Flink)

## 5.1 배치 vs 스트림 처리

### 쉬운 설명

- **배치 처리**: 데이터 **모아서** 한 번에 처리 (세탁기 돌리기)
- **스트림 처리**: 데이터 **들어오는 대로** 처리 (흐르는 물에 손 씻기)

### 비교

| 특성 | 배치 | 스트림 |
|------|------|--------|
| 레이턴시 | 높음 (시간~일) | 낮음 (밀리초~초) |
| 처리량 | 매우 높음 | 높음 |
| 복잡도 | 단순 | 복잡 (상태 관리) |
| 예시 | 일별 집계, ETL | 실시간 대시보드, 알림 |

### Flink의 장점

1. **Exactly-Once**: 장애 시에도 정확히 한 번 처리 보장
2. **상태 관리**: 윈도우, 집계 상태 자동 관리
3. **이벤트 시간**: 데이터의 실제 발생 시간 기준 처리

---

## 5.2 윈도우(Window)

### 쉬운 설명

스트리밍 데이터를 **시간/개수로 묶어서** 처리

### 윈도우 종류

```
1. Tumbling Window (고정 윈도우)
|----1분----|----1분----|----1분----|
[  집계1   ][  집계2   ][  집계3   ]

2. Sliding Window (슬라이딩 윈도우)
|----1분----|
    |----1분----|
        |----1분----|
(30초마다 1분 집계)

3. Session Window (세션 윈도우)
|--활동--|  갭  |--활동--|  갭  |--활동--|
[ 세션1 ]      [ 세션2 ]      [ 세션3 ]
```

### Flink SQL 예시

```sql
-- 1분마다 평균가 계산
SELECT
    market,
    TUMBLE_START(event_time, INTERVAL '1' MINUTE) as window_start,
    AVG(trade_price) as avg_price
FROM ticker_stream
GROUP BY market, TUMBLE(event_time, INTERVAL '1' MINUTE);
```

---

## 5.3 Watermark와 이벤트 시간

### 쉬운 설명

- **이벤트 시간**: 데이터가 **실제로 발생한 시간**
- **처리 시간**: 데이터가 **서버에 도착한 시간**
- **워터마크**: "이 시간까지의 데이터는 다 왔다"는 **신호**

### 왜 필요한가?

```
실제 발생 순서: A(10:00) → B(10:01) → C(10:02)
도착 순서:      B(10:01) → C(10:02) → A(10:00)  ← 네트워크 지연

처리 시간 기준: B, C 처리 → A 처리 (순서 어긋남)
이벤트 시간 기준: A, B, C 순서대로 처리 (정확)
```

### 워터마크 동작

```
데이터 도착: [10:00], [10:02], [10:01], [10:03]
                                         ↑
                              Watermark = 10:02
                              (10:02 이전 데이터 처리 가능)
```

---

# 6. 데이터베이스 (ClickHouse)

## 6.1 OLTP vs OLAP

### 쉬운 설명

- **OLTP**: 주문, 결제 같은 **트랜잭션** 처리 (MySQL, PostgreSQL)
- **OLAP**: 통계, 분석 같은 **대량 조회** 처리 (ClickHouse, BigQuery)

### 비교

| 특성 | OLTP | OLAP |
|------|------|------|
| 쿼리 | 단순, 소량 | 복잡, 대량 |
| 행/열 | 행 기반 | 열 기반 |
| 인덱스 | B-Tree | 희소 인덱스 |
| 정규화 | 높음 | 낮음 (비정규화) |

### 열 기반 저장의 장점

```
행 기반 (Row-oriented)
┌────┬───────────┬────────┬─────────┐
│ ID │ market    │ price  │ volume  │
├────┼───────────┼────────┼─────────┤
│ 1  │ KRW-BTC   │ 50000  │ 1.0     │
│ 2  │ KRW-ETH   │ 2000   │ 10.0    │
│ 3  │ KRW-BTC   │ 50100  │ 0.5     │
└────┴───────────┴────────┴─────────┘
→ 모든 열 읽어야 함

열 기반 (Column-oriented)
market: [KRW-BTC, KRW-ETH, KRW-BTC, ...]
price:  [50000, 2000, 50100, ...]
volume: [1.0, 10.0, 0.5, ...]
→ 필요한 열만 읽음

SELECT AVG(price) FROM ticker;
→ price 열만 스캔 (10x 빠름)
```

---

## 6.2 ClickHouse 테이블 엔진

### MergeTree (가장 기본)

```sql
CREATE TABLE ticker_local (
    event_time DateTime,
    market String,
    trade_price Float64
) ENGINE = MergeTree()
ORDER BY (market, event_time)  -- 정렬 키 (인덱스)
TTL event_time + INTERVAL 7 DAY;  -- 7일 후 삭제
```

**특징**:
- 데이터 파트 단위로 저장
- 백그라운드에서 파트 병합 (Merge)
- 정렬 키로 효율적 조회

### Kafka Engine

```sql
CREATE TABLE ticker_queue (
    ...
) ENGINE = Kafka
SETTINGS
    kafka_broker_list = 'kafka:9092',
    kafka_topic_list = 'upbit-ticker',
    kafka_group_name = 'clickhouse-consumer';
```

**특징**:
- Kafka에서 직접 데이터 읽기
- Materialized View로 MergeTree에 저장

### 면접 포인트

> **Q: ClickHouse가 빠른 이유는?**
> 
> A: 네 가지 이유입니다.
> 1. 열 기반 저장 - 필요한 열만 읽음
> 2. 벡터화 실행 - SIMD로 배치 처리
> 3. 데이터 압축 - 같은 열은 압축률 높음
> 4. 희소 인덱스 - 8192행당 하나의 인덱스

---

## 6.3 분산 테이블

### 구조

```
                Distributed Table (가상)
                    upbit.ticker
                        │
        ┌───────────────┼───────────────┐
        ↓               ↓               ↓
   MergeTree       MergeTree       MergeTree
   (Shard 1)       (Shard 2)       (Shard 3)
  ticker_local    ticker_local    ticker_local
```

### 설정

```sql
CREATE TABLE ticker ON CLUSTER my_cluster
ENGINE = Distributed(
    my_cluster,      -- 클러스터 이름
    upbit,           -- 데이터베이스
    ticker_local,    -- 로컬 테이블
    rand()           -- 샤딩 키
);
```

---

# 7. 객체 스토리지 (MinIO + Paimon)

## 7.1 객체 스토리지 vs 파일 시스템

### 쉬운 설명

- **파일 시스템**: 폴더 구조 (C:\Users\Documents\file.txt)
- **객체 스토리지**: 평평한 구조 (bucket/key)

### 비교

| 특성 | 파일 시스템 | 객체 스토리지 |
|------|------------|--------------|
| 구조 | 계층적 | 평면적 |
| 접근 | POSIX API | HTTP API |
| 확장성 | 제한적 | 무제한 |
| 메타데이터 | 고정 | 사용자 정의 |
| 사용처 | 로컬 | 클라우드, 빅데이터 |

### S3 호환 API

```python
# MinIO는 S3 API 완전 호환
import boto3

client = boto3.client(
    's3',
    endpoint_url='http://localhost:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin'
)

# 파일 업로드
client.put_object(Bucket='paimon', Key='data/file.parquet', Body=data)
```

---

## 7.2 Apache Paimon

### 쉬운 설명

객체 스토리지 위에서 **데이터베이스처럼** 사용할 수 있게 해주는 테이블 포맷

### 특징

1. **스트리밍 + 배치**: Flink와 네이티브 연동
2. **ACID 트랜잭션**: 데이터 일관성 보장
3. **Time Travel**: 과거 버전 조회 가능
4. **Upsert**: 실시간 업데이트 지원

### 구조

```
MinIO Bucket: paimon/
├── ticker/
│   ├── manifest/      -- 메타데이터
│   ├── data/          -- 실제 데이터 파일
│   └── snapshot/      -- 스냅샷 정보
```

---

# 8. 컨테이너와 오케스트레이션

## 8.1 Docker 기본

### 쉬운 설명

- **컨테이너**: 앱과 환경을 **패키징**한 것 (이사 박스)
- **이미지**: 컨테이너의 **설계도** (이사 박스 조립 설명서)
- **Docker**: 컨테이너 **실행 엔진**

### 가상 머신과의 차이

```
가상 머신                     컨테이너
┌─────────────────────┐     ┌─────────────────────┐
│ App A   │ App B     │     │ App A   │ App B     │
├─────────┼───────────┤     ├─────────┼───────────┤
│ Guest OS│ Guest OS  │     │ Container Runtime   │
├─────────┴───────────┤     ├─────────────────────┤
│    Hypervisor       │     │      Host OS        │
├─────────────────────┤     ├─────────────────────┤
│    Host OS          │     │    Hardware         │
├─────────────────────┤     └─────────────────────┘
│    Hardware         │
└─────────────────────┘
      무거움                      가벼움
```

### Docker Compose

```yaml
# 여러 컨테이너를 한 번에 관리
services:
  kafka-1:
    image: confluentinc/cp-kafka:7.5.1
    networks:
      - pipeline-network
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    depends_on:
      - kafka-2
```

---

## 8.2 Kubernetes 기본

### 쉬운 설명

많은 컨테이너를 **자동으로 관리**해주는 시스템 (컨테이너 오케스트라 지휘자)

### 핵심 개념

```
┌─────────────────────────────────────────────────────┐
│                    Kubernetes Cluster                │
│  ┌────────────────────────────────────────────────┐ │
│  │                Control Plane                    │ │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────────┐   │ │
│  │  │API Server│ │Scheduler │ │Controller    │   │ │
│  │  └──────────┘ └──────────┘ │Manager       │   │ │
│  │                            └──────────────┘   │ │
│  └────────────────────────────────────────────────┘ │
│                                                     │
│  ┌─────────────────┐  ┌─────────────────┐          │
│  │    Node 1       │  │    Node 2       │          │
│  │  ┌───────────┐  │  │  ┌───────────┐  │          │
│  │  │   Pod     │  │  │  │   Pod     │  │          │
│  │  │ ┌───────┐ │  │  │  │ ┌───────┐ │  │          │
│  │  │ │Container│ │  │  │ │Container│ │  │          │
│  │  │ └───────┘ │  │  │  │ └───────┘ │  │          │
│  │  └───────────┘  │  │  └───────────┘  │          │
│  └─────────────────┘  └─────────────────┘          │
└─────────────────────────────────────────────────────┘

Pod: 컨테이너 그룹 (최소 배포 단위)
Node: 실제 서버 (물리/가상)
Service: 파드에 접근하는 방법 (로드밸런서)
Deployment: 파드 배포 관리
```

### 이 프로젝트의 K8s 설정

```yaml
# kubernetes/kafka.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    spec:
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:7.5.1
        resources:
          requests:
            memory: "512Mi"
          limits:
            memory: "1Gi"
```

---

# 9. Python 심화

## 9.1 Python GIL (Global Interpreter Lock)

### 쉬운 설명

Python 인터프리터의 **잠금 장치**. 한 번에 하나의 스레드만 Python 코드 실행 가능.

### 왜 있나?

- CPython 메모리 관리(참조 카운팅)가 스레드 안전하지 않음
- GIL로 간단하게 해결

### 영향

```python
# CPU 바운드: GIL 때문에 병렬화 안 됨
def cpu_bound():
    return sum(i*i for i in range(10**7))

# 멀티스레딩 → 성능 향상 없음 (오히려 오버헤드)
# 멀티프로세싱 → 성능 향상됨 (별도 인터프리터)

# I/O 바운드: GIL 해제됨
def io_bound():
    response = requests.get(url)  # 네트워크 대기 중 GIL 해제
```

### 이 프로젝트에서의 해결

```python
# asyncio 사용 (GIL 영향 없음)
async def _listen(self) -> None:
    async for message in self._websocket:
        await self._handle_message(message)
```

---

## 9.2 데코레이터 패턴

### 쉬운 설명

함수를 **감싸서** 기능을 추가하는 패턴

### 예시

```python
# 캐싱 데코레이터
@lru_cache(maxsize=1)
def get_config() -> AppConfig:
    return AppConfig()

# 동등한 코드
def get_config() -> AppConfig:
    return AppConfig()
get_config = lru_cache(maxsize=1)(get_config)
```

### 이 프로젝트에서

```python
from functools import lru_cache

@lru_cache(maxsize=1)
def get_config() -> AppConfig:
    """설정을 한 번만 로드"""
    return AppConfig()
```

---

## 9.3 싱글톤 패턴

### 쉬운 설명

클래스의 인스턴스를 **하나만** 만드는 패턴

### 이 프로젝트에서

```python
class UpbitKafkaProducer:
    _instance: Optional["UpbitKafkaProducer"] = None
    _lock: Lock = Lock()
    
    def __new__(cls, config=None) -> "UpbitKafkaProducer":
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:  # Double-checked locking
                    instance = super().__new__(cls)
                    instance._initialized = False
                    cls._instance = instance
        return cls._instance
```

**왜 싱글톤?**
- Kafka Producer 연결은 비용이 큼
- 여러 개 만들면 리소스 낭비
- 전역적으로 하나만 필요

### 면접 포인트

> **Q: 싱글톤의 단점은?**
> 
> A: 테스트 어려움 (전역 상태), 숨겨진 의존성, 멀티스레드 시 
> 동기화 필요. 대안으로 의존성 주입(DI)을 사용합니다.

---

## 9.4 Dataclass

### 쉬운 설명

데이터를 담는 클래스를 **자동 생성**해주는 기능

### 비교

```python
# 일반 클래스
class TickerOld:
    def __init__(self, market, price, volume):
        self.market = market
        self.price = price
        self.volume = volume
    
    def __repr__(self):
        return f"Ticker(market={self.market}, price={self.price})"
    
    def __eq__(self, other):
        return (self.market == other.market and 
                self.price == other.price and 
                self.volume == other.volume)

# dataclass
@dataclass
class Ticker:
    market: str
    price: float
    volume: float
    # __init__, __repr__, __eq__ 자동 생성
```

### 옵션

```python
@dataclass(frozen=True)   # 불변 객체 (해시 가능)
@dataclass(slots=True)    # 메모리 최적화 (Python 3.10+)
@dataclass(order=True)    # 비교 연산자 자동 생성
```

---

# 10. 실무 운영 튜닝

## 10.1 Kafka 운영 튜닝

### 처리량 최적화

```python
# Producer 설정
"linger.ms": 20,           # 배치 대기 시간 늘리기 (처리량↑)
"batch.size": 65536,       # 배치 크기 늘리기 (64KB)
"compression.type": "lz4", # 빠른 압축

# Consumer 설정
"fetch.min.bytes": 1048576,      # 최소 1MB 가져오기
"fetch.max.wait.ms": 500,        # 최대 500ms 대기
"max.poll.records": 500,         # 한 번에 500개
```

### 레이턴시 최적화

```python
# Producer 설정
"linger.ms": 0,            # 즉시 전송
"acks": 1,                 # 리더만 확인 (위험 있음)

# Consumer 설정
"fetch.min.bytes": 1,      # 즉시 가져오기
```

### 메모리 최적화

```yaml
# Kafka Broker
KAFKA_HEAP_OPTS: "-Xmx512M -Xms512M"

# 세그먼트 관리
log.segment.bytes: 100000000       # 100MB 세그먼트
log.retention.hours: 168           # 7일 보관
log.retention.bytes: 1073741824    # 최대 1GB
```

---

## 10.2 ClickHouse 운영 튜닝

### 쿼리 최적화

```sql
-- 나쁜 쿼리: 전체 스캔
SELECT * FROM ticker WHERE trade_price > 50000000;

-- 좋은 쿼리: 인덱스 활용
SELECT * FROM ticker 
WHERE market = 'KRW-BTC' 
  AND event_time >= now() - INTERVAL 1 HOUR;
  -- ORDER BY 키 (market, event_time) 활용
```

### 메모리 설정

```xml
<!-- config.xml -->
<max_memory_usage>2000000000</max_memory_usage>  <!-- 2GB -->
<max_memory_usage_for_user>1500000000</max_memory_usage_for_user>
<max_bytes_before_external_group_by>1000000000</max_bytes_before_external_group_by>
```

### 배치 삽입

```sql
-- 배치 삽입 설정
SET max_insert_block_size = 1048576;  -- 1M 행
SET min_insert_block_size_rows = 1048576;
SET min_insert_block_size_bytes = 268435456;  -- 256MB
```

---

## 10.3 Python 애플리케이션 튜닝

### 프로파일링

```python
import cProfile
import pstats

# 성능 측정
with cProfile.Profile() as pr:
    process_messages(1000)

stats = pstats.Stats(pr)
stats.sort_stats('cumulative')
stats.print_stats(10)
```

### 메모리 프로파일링

```python
from memory_profiler import profile

@profile
def process_data():
    data = [i for i in range(1000000)]  # 메모리 사용량 표시
    return sum(data)
```

### 최적화 라이브러리

```python
# JSON 직렬화: 10x 빠름
import orjson
data = orjson.dumps(obj)

# 이벤트 루프: 2-4x 빠름
import uvloop
asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

# HTTP 클라이언트: 연결 풀링
import httpx
async with httpx.AsyncClient() as client:
    response = await client.get(url)
```

---

## 10.4 Docker/K8s 리소스 튜닝

### 리소스 제한

```yaml
resources:
  requests:      # 최소 보장
    memory: "512Mi"
    cpu: "250m"
  limits:        # 최대 허용
    memory: "1Gi"
    cpu: "500m"
```

### Java 앱 (Kafka, Flink) 힙 설정

```yaml
env:
  - name: JAVA_OPTS
    value: "-Xmx512m -Xms512m -XX:+UseG1GC"
```

### 헬스체크

```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

---

# 11. 예상 면접 질문

## 11.1 프로젝트 전반

### Q1: 이 프로젝트에서 가장 어려웠던 점은?

> **모범 답안**: 16GB M1 맥북에서 분산 시스템을 운영하며 메모리 제약을 
> 해결하는 것이었습니다. ClickHouse 2샤드 구성 시 메모리 부족으로 
> 쿼리가 실패했고, 단일 노드로 변경하면서 개발 환경과 운영 환경의 
> 차이를 문서화했습니다. 이를 통해 리소스 제약 하에서의 트레이드오프 
> 결정을 배웠습니다.

### Q2: 왜 이 기술 스택을 선택했나요?

> **모범 답안**: 
> - **Kafka**: 재처리 가능한 로그 기반 아키텍처, 업계 표준
> - **Flink**: Exactly-once 보장, 이벤트 시간 처리
> - **ClickHouse**: 열 기반으로 분석 쿼리 최적화
> - **Paimon**: Flink 네이티브 연동, 스트리밍 레이크하우스
>
> 각각의 대안(Pulsar, Spark Streaming, Druid 등)과 비교 분석 후 
> 우리 요구사항에 맞는 것을 선택했습니다.

---

## 11.2 Kafka 관련

### Q3: Kafka에서 메시지 순서를 어떻게 보장하나요?

> **모범 답안**: Kafka는 파티션 내에서만 순서를 보장합니다. 
> 같은 market(예: KRW-BTC)의 메시지가 순서대로 처리되어야 하므로 
> market을 파티션 키로 사용했습니다. 이렇게 하면 같은 코인의 
> 모든 메시지가 같은 파티션에 들어가고, 컨슈머가 순서대로 처리합니다.

**꼬리질문: 그럼 전체 순서는 어떻게?**

> 전체 순서가 필요하면 파티션을 1개만 사용해야 하지만, 이러면 
> 처리량이 제한됩니다. 대안으로 타임스탬프 기반 정렬이나 
> 시퀀스 번호를 사용합니다.

### Q4: Consumer Group이 왜 필요한가요?

> **모범 답안**: 수평 확장을 위해서입니다. 같은 Consumer Group의 
> 컨슈머들은 파티션을 나눠서 처리합니다. 컨슈머를 늘리면 
> 처리량이 증가합니다. 단, 컨슈머 수가 파티션 수를 초과하면 
> 놀게 되므로, 파티션 수 ≥ 컨슈머 수로 설계합니다.

---

## 11.3 데이터 처리 관련

### Q5: Exactly-once 처리를 어떻게 보장하나요?

> **모범 답안**: 세 가지 레벨에서 보장합니다.
> 1. **Kafka Producer**: idempotent producer (enable.idempotence=true)
> 2. **Kafka Consumer**: 오프셋 커밋과 처리를 트랜잭션으로
> 3. **Flink**: 체크포인트 + 2PC (Two-Phase Commit)
>
> 이 프로젝트에서는 acks=all과 idempotent producer로 
> 프로듀서 측 중복을 방지하고, ClickHouse의 ReplacingMergeTree로 
> 컨슈머 측 중복을 처리합니다.

### Q6: 데이터 파이프라인에서 백프레셔(Backpressure)를 어떻게 처리하나요?

> **모범 답안**: 
> - **Kafka**: producer 버퍼가 차면 block 또는 예외 발생
> - **Flink**: 자동 백프레셔 (다운스트림이 느리면 업스트림 속도 조절)
> - **WebSocket**: 메시지 처리가 느리면 버퍼링, 버퍼 초과 시 연결 끊김
>
> 이 프로젝트에서는 Kafka producer의 BufferError를 캐치해서 
> poll(1)로 1초 대기 후 재시도합니다.

---

## 11.4 성능 관련

### Q7: 이 시스템의 예상 처리량은?

> **모범 답안**: 단일 Python 프로세스 기준:
> - WebSocket 수신: ~10,000 msg/s
> - 메시지 파싱 + Kafka 전송: ~5,000 msg/s
> - 병목: JSON 직렬화/역직렬화
>
> 처리량 향상을 위해서는:
> 1. 멀티프로세싱 (마켓별 분리)
> 2. orjson 사용 (10x 빠른 JSON)
> 3. Protocol Buffers 도입

### Q8: ThreadPoolExecutor를 왜 제거했나요?

> **모범 답안**: librdkafka(Kafka 클라이언트)가 이미 내부적으로 
> 비동기 배치 처리를 합니다. produce()는 버퍼에 추가만 하고 
> 즉시 반환됩니다. Python 스레드풀을 추가하면 컨텍스트 스위칭 
> 오버헤드(50-100μs/call)만 발생합니다.
>
> 제거 후 ~30% 성능 향상을 확인했습니다.

---

## 11.5 시스템 설계 관련

### Q9: 시스템에 장애가 발생하면 어떻게 되나요?

> **모범 답안**: 각 컴포넌트별로:
>
> - **WebSocket 연결 끊김**: 지수 백오프로 재연결 (최대 10회)
> - **Kafka 브로커 다운**: 3개 중 1개 다운 시 자동 페일오버
> - **ClickHouse 다운**: Grafana에서 에러 표시, 데이터는 Kafka에 보관
>
> 전체 복구:
> 1. Kafka에 7일간 데이터 보관
> 2. ClickHouse 복구 후 오프셋부터 재처리

### Q10: 확장이 필요하면 어떻게 하나요?

> **모범 답안**:
>
> **수평 확장**:
> - WebSocket Client: 마켓별로 분리 (Python 멀티프로세싱)
> - Kafka: 파티션 수 증가, 브로커 추가
> - ClickHouse: 샤드 추가
>
> **수직 확장**:
> - 메모리 증설 (ClickHouse 버퍼)
> - 더 빠른 SSD
>
> 현재 아키텍처는 각 레이어가 독립적이라 개별 확장이 가능합니다.

---

## 11.6 Python/코드 관련

### Q11: 왜 asyncio를 사용했나요?

> **모범 답안**: WebSocket은 I/O 바운드 작업입니다. 메시지가 언제 
> 올지 모르고, 대부분의 시간을 대기합니다. asyncio를 사용하면:
> 1. 단일 스레드로 수천 연결 처리 가능
> 2. GIL 영향 없음
> 3. 코루틴은 스레드보다 1000배 가벼움 (KB vs MB)

### Q12: 싱글톤 패턴을 왜 사용했나요?

> **모범 답안**: Kafka Producer는 연결 생성 비용이 크고, 
> 내부적으로 배치 처리를 위한 상태를 유지합니다. 여러 개 
> 만들면 리소스 낭비이고, 배치 효율도 떨어집니다.
>
> 단점(테스트 어려움)은 테스트에서 싱글톤 인스턴스를 리셋하는 
> fixture로 해결했습니다.

---

## 11.7 빅테크 스타일 심화 질문

### Q13: 초당 100만 메시지를 처리하려면 어떻게 설계하나요?

> **모범 답안**:
>
> **아키텍처**:
> 1. WebSocket → 여러 수집 서버로 분산 (마켓별)
> 2. Kafka 파티션 100개+, 브로커 10개+
> 3. Flink 작업 병렬도 증가
> 4. ClickHouse 클러스터 (10+ 샤드)
>
> **최적화**:
> 1. Protocol Buffers로 직렬화
> 2. 배치 크기 증가 (1MB+)
> 3. 압축 (lz4 또는 zstd)
>
> **예상 리소스**:
> - 수집 서버: 10대 (각 10만 msg/s)
> - Kafka: 10 브로커 × 32GB RAM
> - ClickHouse: 10 샤드 × 64GB RAM

### Q14: 데이터 일관성과 성능 중 어떤 것을 선택하나요?

> **모범 답안**: 비즈니스 요구사항에 따라 다릅니다.
>
> **일관성 우선** (금융 거래):
> - acks=all, 동기 복제
> - Exactly-once 처리
> - 처리량 감소 감수
>
> **성능 우선** (실시간 분석):
> - acks=1, 비동기 복제
> - At-least-once + 멱등성
> - 간헐적 중복 허용
>
> 이 프로젝트는 시세 데이터라 최종 일관성으로 충분합니다.
> 같은 시간의 데이터가 잠시 다를 수 있지만, 분석에는 지장 없습니다.

---

## 정리

이 문서에서 다룬 핵심 개념들:

| 영역 | 핵심 개념 |
|------|----------|
| CS 기초 | 동기/비동기, 블로킹/논블로킹, 스레드/프로세스, 복잡도 |
| 네트워크 | HTTP/WebSocket, TCP/UDP, 3-way handshake |
| 분산 시스템 | CAP, 일관성 모델, 복제, 파티셔닝 |
| Kafka | 토픽/파티션, Producer/Consumer, acks, KRaft |
| Flink | 배치/스트림, 윈도우, 워터마크 |
| ClickHouse | OLTP/OLAP, 열 기반 저장, 분산 테이블 |
| Python | GIL, asyncio, 데코레이터, 싱글톤 |
| 운영 | 튜닝, 모니터링, 장애 대응, 확장 |

면접에서는 **"왜"**를 항상 설명할 수 있어야 합니다.
기술 선택의 이유, 트레이드오프, 대안까지 준비하세요.
